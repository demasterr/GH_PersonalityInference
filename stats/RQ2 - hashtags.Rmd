---
title: "Research Question 2 - hashtags"
author: "Frenk C.J. van Mil"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r, result='hide', echo=FALSE, include=FALSE}
source('statistics.R')
```

```{r, results='hide', echo=FALSE, include=FALSE}
transformations <- list(
  c(13, log10_transform)
)

dbs_res <- readdbs('selected_dev_parsed', 'selected_dev_parsed_hashtags', transformations=transformations)

data_og <- dbs_res$db1
data_cmp <- dbs_res$db2

dfs <- list(data_og, data_cmp)
dfnames <- c('Enabled', 'Disabled')
method <- "hashtags"
method_full <- "hashtag"
rm(dbs_res)
```

In order to evaluate research question 2, we need to connect to the MySQL database first. From the database we fetch to dataframes, _data\_og_ (scores with all preprocessing steps enabled) and _data\_cmp_ (scores with hashtags). Any column indicating significance (signf) means a `p-value` below `0.05` is found.


## Summarize data
```{r}
summarize_df(dfs, dfnames=dfnames, fix_naming=TRUE)
```

From the summary of above, we can see that some columns do not seem to be affected (much). We should check, however, if this is significant or not.

Furthermore, by looking at the median and the mean, we can see that it seems that most column are not too skewed to one side. Therefore, normallity may be possible.

First we check if there even is a difference in a single row and column.

```{r}
inspect_AE_differences(df1 = data_og, df2 = data_cmp)
```

```{r}
latex_table(func=inspect_AE_differences, 
            caption=get_caption("summarize",method_full),
            label=paste0('tab:', method,'_diff'),
            include_index = FALSE,
            fix_column = 1,
            centering = TRUE,
            df1 = data_og, df2 = data_cmp)
```

First we will look if the distributions are normal or not. To visualize this, we draw histograms, density plots, and QQ-plots.

```{r}
hist_all(data_og, data_cmp, dfname=method_full)
```

```{r}
density_all(data_og, data_cmp, dfname=method_full)
```

```{r, cache=TRUE}
draw_qqplots(data_og, data_cmp)
```

By looking at the QQ-plots, we can see that some appear at least near-normally distributed. However, most do not seem to be normally distributed.


## Shapiro-Wilk Normality test
To make check our findings of the QQ-plots, we could use Shapiro-Wilk normality testing to check if the traits are significantly non-normal.
```{r}
sample_size <- 50
repeats <- 1000
shap_og <- shap(data_og, sample_size=sample_size)
print(shap_og)

shap_cmp <- shap(data_cmp, sample_size=sample_size)
print(shap_cmp)

repeated(shap, repeats, data_og, sample_size) # Check if the p-values are most of the time right.
repeated(shap, repeats, data_og, sample_size=sample_size) # Check if the p-values are most of the time right.
```

```{r}
normal_cols <- c(1,3:11,15)
non_normal_cols <- c(1,2,12:14,16)
```

From the above we can conclude that the data is significanlty different from a normal distribution for all columns.

## Fisher test (compare variances)
Before we can compare whether there is a difference in means, we first need to compare the variances.

```{r}
fisherf_df <- fisherf(data_og, data_cmp)
fisherf_df
```
From the Fisher F test, we could not find any significant difference between the personality traits. Therefore, we could not lay any strong conclusions here.

## Wilcox signed rank test with continuaty correction (paired = TRUE)
Paired, as both are continuous.
```{r}
wilc_df <- wilc(data_og, data_cmp, paired = TRUE)
wilc_df
```

Because we found all columns to be significantly different from a normal distribution, we use the Wilcox rank sum test. The Wilcox rank sum test should show us under the hypothesis that the two means are the same. We assume that the pairing is needed, as the scores are drawn for the same person but with different parsing.
Only for Yarkoni extraversion we have a p-value showing a significant difference in mean trait values for the original data and without hashtag parsing. For all other columns, we do not have enough evidence to reject the null hypothesis.

## T-test
```{r}
ttest_df <- ttest(data_og, data_cmp)
ttest_df
```

If the data would be normally distributed, we could also have used the t-test. However, we found earlier this is not the case.
Still, if we execute the code, we can see the same results as above for the non-parametric methods.

## Effect size mean - Cohen's d
```{r}
cohensd_df <- cohensd(data_og, data_cmp)
cohensd_df
```

```{r}
latex_table(func=wilc, caption=get_caption("wilc",method_full), label=paste0("tab:",method,"_wilcox"), df1=data_og[,non_normal_cols], df2=data_cmp[,non_normal_cols], paired=TRUE)
```


```{r}
latex_table(func=ttest_eff, caption=get_caption("ttest",method_full), label=paste0("tab:",method,"_ttest"), df1=data_og[,normal_cols], df2=data_cmp[,normal_cols], paired=TRUE)
```

## Plots
```{r}
plot_line_ordered(data_og, data_cmp)
```

```{r}
trait_boxplots(data_og, data_cmp)
```

```{r}
ggplot_line_ordered(data_og, data_cmp)
```

```{r}
gt <- get_ground_truth()

insp <- inspect_gt_non_zero(data_og, data_cmp, dfnames=dfnames, gt=gt)
insp
```
From the above we can conclude there is no real difference to be observed overall.

```{r}
diff_cols <- get_diff_cols(insp)

latex_table(inspect_gt, caption=get_caption("accuracy",method_full), label=paste0("tab:",method,"_gt_RMSE_MAE"),
            include_index=FALSE,
            shorten_names=1,
            fix_column=1,
            transpose=TRUE,
            alignment="c",
            df1=data_og, df2=data_cmp, dfnames=dfnames, gt=gt, columns=diff_cols)
```

```{r}
measure_plot(dfs=list(data_og, data_cmp), dfnames=list('og', 'cmp'), gt=gt, func=MAE, main='MAE for original scores compared to code block scores.', truncate_columns = TRUE, show_legend=TRUE, legend_title='Dataset', size_in_legend=FALSE)
```
