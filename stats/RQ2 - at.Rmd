---
title: "Research Question 2 - at"
author: "Frenk C.J. van Mil"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r, result='hide', echo=FALSE, include=FALSE}
source('statistics.R')
```

```{r, results='hide', echo=FALSE, include=FALSE}
transformations <- list(
  c(13, log10_transform)
)
dbs_res <- readdbs('selected_dev_parsed', 'selected_dev_parsed_at', transformations = transformations)

data_og <- dbs_res$db1
data_cmp <- dbs_res$db2

dfs <- list(data_og, data_cmp)
dfnames <- c('Enabled', 'Disabled')
method <- "at"
method_full <- "@-reference"
rm(dbs_res)
```

In order to evaluate research question 2, we need to connect to the MySQL database first. From the database we fetch to dataframes, _data\_og_ (scores with all preprocessing steps enabled) and _data\_cmp_ (scores without at (@) parsing). Any column indicating significance (signf) means a `p-value` below `0.05` is found.

## Summarize data
```{r}
summarize_df(dfs, dfnames=dfnames, fix_naming=TRUE)
```

From the summary of above, we can see that some columns do not seem to be affected (much). For example, PI extraversion and neuroticism are not really affected by the parsing. However, all other columns of all methods seem to be affected at least some. We should check, however, if this is significant or not.

Furthermore, by looking at the median and the mean, we can see that it seems that most column are not too skewed to one side. Therefore, normallity may be possible.

First we check if there even is a difference in a single row and column.

```{r}
inspect_AE_differences(df1 = data_og, df2 = data_cmp)
```

```{r}
latex_table(func=inspect_AE_differences, 
            caption=get_caption("summarize",method_full),
            label=paste0('tab:', method,'_diff'),
            include_index = FALSE,
            fix_column = 1,
            df1 = data_og, df2 = data_cmp)
```


First we will look if the distributions are normal or not. To visualize this, we draw histograms, density plots, and QQ-plots.

```{r}
hist_all(data_og, data_cmp, dfname=method_full)
```

```{r}
density_all(data_og, data_cmp, dfname=method_full)
```

```{r, cache=TRUE}
draw_qqplots(data_og, data_cmp)
```

By looking at the QQ-plots, we can see that some appear at least near-normally distributed. However, most do not seem to be normally distributed.


## Shapiro-Wilk Normality test
To make check our findings of the QQ-plots, we could use Shapiro-Wilk normality testing to check if the traits are significantly non-normal.
```{r}
sample_size <- 50
repeats <- 1000
shap_og <- shap(data_og, sample_size=sample_size)
print(shap_og)

shap_cmp <- shap(data_cmp, sample_size=sample_size)
print(shap_cmp)

repeated(shap, repeats, data_og, sample_size) # Check if the p-values are most of the time right.
repeated(shap, repeats, data_og, sample_size=sample_size) # Check if the p-values are most of the time right.
```


From the above we can conclude that the data is significanlty different from a normal distribution for the following columns:
- PI:
  - Openness
- Golbeck:
  - Openness
  - Extraversion
  - Neuroticism
  
```{r}
normal_cols <- c(1,3:15)
non_normal_cols <- c(1,2,16)
```

## Fisher test (compare variances)
Before we can compare whether there is a difference in means, we first need to compare the variances.

```{r}
fisherf_df <- fisherf(data_og, data_cmp)
fisherf_df
```
From the Fisher F test, we could find a significant different for all personality traits in the variance. We should keep in mind this may be caused by outliers. However, by looking at the illustrations, this does not necessarily seem to be the case. The larger the value F, the larger the difference in variance is.

Because we have already done this test, it wouldn't be necessary to do tests for the difference in means.

## Wilcox signed rank test with continuaty correction (paired = TRUE)
```{r}
wilc_df <- wilc(data_og[,non_normal_cols], data_cmp[,non_normal_cols], paired = TRUE, p = 0.05)
wilc_df
```

Because we found all columns to be significantly different from a normal distribution, we use the Wilcox rank sum test. The Wilcox rank sum test should show us under the hypothesis that the two means are the same. We assume that the pairing is needed, as the scores are drawn for the same person but with different parsing.
For all columns, we reject the null hypothesis and conclude that the mean trait values for the original data and without code parsing are significantly different.

## T-test
```{r}
ttest_df <- ttest(data_og[,normal_cols], data_cmp[,normal_cols], p=0.05)
ttest_df
```

If the data would be normally distributed, we could also have used the t-test. However, we found earlier this is not the case.
Still, if we execute the code, we can see there is not contradiction on all columns, except for `pi_extraversion`. This is not unexpected as the p-value for t-test is usually lower than the Wilcox ranked sum test p-value.
From the above, we could have concluded that the means of the two sets of data are significantly different from each other for all personality traits.

## Effect size mean - Cohen's d
```{r}
cohensd_df <- cohensd(data_og[,normal_cols], data_cmp[,normal_cols])
cohensd_df
```

```{r}
latex_table(func=wilc, caption=get_caption("wilc",method_full), label=paste0("tab:",method,"_wilcox"), df1=data_og[,non_normal_cols], df2=data_cmp[,non_normal_cols], paired=TRUE)
```

```{r}
latex_table(func=ttest_eff, caption=get_caption("ttest",method_full), label=paste0("tab:",method,"_ttest"), df1=data_og[,normal_cols], df2=data_cmp[,normal_cols], paired=TRUE)
```

## Plots
```{r}
plot_line_ordered(data_og, data_cmp)
```

```{r}
trait_boxplots(data_og, data_cmp)
```

```{r}
ggplot_line_ordered(data_og, data_cmp)
```



```{r}
gt <- get_ground_truth()

insp <- inspect_gt_non_zero(data_og, data_cmp, dfnames=dfnames, gt=gt)
insp
```

```{r}
diff_cols <- get_diff_cols(insp)

latex_table(inspect_gt_non_zero, caption=get_caption("accuracy_non_zero",method_full), label=paste0("tab:",method,"_gt_RMSE_MAE"),
            include_index=FALSE,
            fix_column=1,
            transpose=TRUE,
            alignment="c",
            df1=data_og, df2=data_cmp, dfnames=dfnames, gt=gt, columns=diff_cols)
```

```{r}
compare_metrics(data_og, data_cmp, dfnames=dfnames, gt=gt)
```

```{r}
difference_plot <- function(df, gt) {
  df <- df[df$id %in% gt$id,]
  gt <- gt[gt$id %in% df$id,]
  
  merged <- merge(df, gt[1:6], by="id")
  for (i in 1:5) {
    golb_idx <- i + 11
    gt_idx <- i + 16
    
    ordered_merged <- merged[order(merged[,gt_idx], merged[,golb_idx]),]

    plot(1:nrow(ordered_merged), ordered_merged[,gt_idx], type='n',
         main=paste0('Ordered scores [',traits[[i]],']'),
         xlab='People',
         ylab='Absolute difference')
    lines(1:nrow(ordered_merged), ordered_merged[,gt_idx], col='black')
    lines(1:nrow(ordered_merged), ordered_merged[,golb_idx], col=color$c3)
    legend(x="topleft", y=0.92, c('gt','pi','yarkoni','golbeck'), col=c('black', color$c1, color$c2, color$c3), pch=rep('-',3), lwd=5)
  }
}
difference_plot(data_og, gt)
```

```{r}
draw_ordered <- function(df1, df2) {
  merged <- inner_join(df1, df2, by='id', copy=TRUE, suffix=c('.x', '.y'))
  for (i in 2:length(df1)) {
    merged <- merged[order(merged[,i]),]
    row.names(merged) <- NULL
    print(paste(colnames(merged)[[i]], colnames(merged)[[i + 15]]))
    
    plot(x=1:nrow(merged), y=merged[,i], 
         type='n', ylab=colnames(df1)[[i]], ylim=c(0,1))
    lines(x=1:nrow(merged), y=merged[,i + 15], col='blue')
    lines(x=1:nrow(merged), y=merged[,i], col='red')
    lines(x=1:nrow(merged), y=merged[,i], col=alpha(rgb(1,0,0), 0.5), lwd=10)
  }
}
draw_ordered(data_og, data_cmp)
```

```{r}
draw_ordered2 <- function(df1, df2) {
  df1 <- df1[df1$id %in% df2$id,]
  df2 <- df2[df2$id %in% df1$id,]
  for (i in 2:length(df1)) {
    ordered_df1 <- df1[order(df1[,i]),]
    row.names(ordered_df1) <- NULL
    ordered_df2 <- df2[order(df2[,i]),]
    row.names(ordered_df2) <- NULL
    
    plot(x=1:nrow(ordered_df1), y=ordered_df1[,i], 
         type='n', ylab=colnames(df1)[[i]], ylim=c(0,1))
    lines(x=1:nrow(ordered_df1), y=ordered_df2[,i], col='blue')
    lines(x=1:nrow(ordered_df1), y=ordered_df1[,i], col='red')
  }
}
draw_ordered2(data_og, data_cmp)
```

```{r}
whos_better(data_og, data_cmp, gt, dfnames=dfnames, allowed_error=0.001)
```

```{r}
for (i in starting_index(data_og):length(data_og)) {
  plot(sort(data_og[,i]), main=colnames(data_og)[[i]])
}
```

```{r}
for (i in starting_index(data_og):length(data_og)) {
  plot(sort(zero_scale(data_og[,i])), main=colnames(data_og)[[i]])
}
```

```{r}
print(data_og)
print(mean(data_og[,2]))
```

```{r}
zero_scale(data_og)
```

```{r}
zero_scale(gt[,1:6])
```

```{r}
compare_metrics(zero_scale(data_og), zero_scale(data_cmp), dfnames=dfnames, gt=zero_scale(gt[,1:6]))
```

```{r}
compare_metrics(data_og, data_cmp, dfnames=dfnames, gt=gt[,1:6])
```

```{r}
print('0')
whos_better(data_og, data_cmp, gt=gt, dfnames=dfnames, allowed_error = 0)
print(0.01)
whos_better(data_og, data_cmp, gt=gt, dfnames=dfnames, allowed_error = 0.01)
print(0.05)
whos_better(data_og, data_cmp, gt=gt, dfnames=dfnames, allowed_error = 0.05)
```

```{r}
whos_better(zero_scale(data_og), zero_scale(data_cmp), gt=gt, dfnames=dfnames, allowed_error = 0)
whos_better(zero_scale(data_og), zero_scale(data_cmp), gt=gt, dfnames=dfnames, allowed_error = 0.001)
whos_better(zero_scale(data_og), zero_scale(data_cmp), gt=gt, dfnames=dfnames, allowed_error = 0.01)
whos_better(zero_scale(data_og), zero_scale(data_cmp), gt=gt, dfnames=dfnames, allowed_error = 0.05)
```
