---
title: "Research Question 2 - emails"
author: "Frenk C.J. van Mil"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r, result='hide', echo=FALSE, include=FALSE}
source('statistics.R')
```

```{r, results='hide', echo=FALSE, include=FALSE}
transformations <- list(
  c(13, log10_transform)
)
dbs_res <- readdbs('selected_dev_parsed', 'selected_dev_parsed_emails', transformations = transformations)

data_og <- dbs_res$db1
data_cmp <- dbs_res$db2

dfs <- list(data_og, data_cmp)
dfnames = c('Enabled', 'Disabled')
method <- "emails"
method_full <- "email"
rm(dbs_res)
```

In order to evaluate research question 2, we need to connect to the MySQL database first. From the database we fetch to dataframes, _data\_og_ (scores with all preprocessing steps enabled) and _data\_cmp_ (scores with emails). Any column indicating significance (signf) means a `p-value` below `0.05` is found.


## Summarize data
```{r}
summarize_df(dfs, dfnames=dfnames, fix_naming=TRUE)
```

From the summary of above, we can see that some columns do not seem to be affected (much). We should check, however, if this is significant or not.

Furthermore, by looking at the median and the mean, we can see that it seems that most column are not too skewed to one side. Therefore, normallity may be possible.

First we check if there even is a difference in a single row and column.

```{r}
inspect_AE_differences(df1 = data_og, df2 = data_cmp)
```

```{r}
latex_table(func=inspect_AE_differences, 
            caption=get_caption("summarize",method_full),
            label=paste0('tab:', method,'_diff'),
            include_index = FALSE,
            fix_column = 1,
            centering = TRUE,
            df1 = data_og, df2 = data_cmp)
```

Then we will look if the distributions are normal or not. To visualize this, we draw histograms, density plots, and QQ-plots.

```{r}
hist_all(data_og, data_cmp, dfname=method_full)
```

```{r}
density_all(data_og, data_cmp, dfname=method_full)
```

```{r, cache=TRUE}
draw_qqplots(data_og, data_cmp)
```

By looking at the QQ-plots, we can see that some appear at least near-normally distributed. However, most do not seem to be normally distributed.


## Shapiro-Wilk Normality test
To make check our findings of the QQ-plots, we could use Shapiro-Wilk normality testing to check if the traits are significantly non-normal.
```{r}
sample_size <- 50
repeats <- 1000
shap_og <- shap(data_og, sample_size=sample_size)
print(shap_og)

shap_cmp <- shap(data_cmp, sample_size=sample_size)
print(shap_cmp)

repeated(shap, repeats, data_og, sample_size) # Check if the p-values are most of the time right.
repeated(shap, repeats, data_og, sample_size=sample_size) # Check if the p-values are most of the time right.
```


From the above we can conclude that the data is significanlty different from a normal distribution for:
- PI:
  - Openness
- Golbeck:
  - Openness
  - Extraversion
  - Neuroticism

For all other columns, we could not find any significant indication of non-normality. We, therefore, normality or at least near-normality for these columns.

```{r}
normal_cols <- c(1,3:11,15)
non_normal_cols <- c(1,2,12:14,16)
```

## Fisher test (compare variances)
Before we can compare whether there is a difference in means, we first need to compare the variances.

```{r}
fisherf_df <- fisherf(data_og, data_cmp)
fisherf_df
```
From the Fisher F test, we could not find any significant difference between the personality traits. Therefore, we could not lay any strong conclusions here. We, therefore, move on to check for differences in the mean.

## Wilcox signed rank test with continuaty correction (paired = TRUE)
```{r}
wilc_df <- wilc(data_og[,non_normal_cols], data_cmp[,non_normal_cols], paired = TRUE, method="wilcoxon-pratt")
wilc_df
```

Because we found some columns to be significantly different from a normal distribution, we use the Wilcox rank sum test. The Wilcox rank sum test should show us under the hypothesis that the two means are the same. We assume that the pairing is needed, as the scores are drawn for the same person but with different parsing.
For all columns, we do not have enough evidence to reject the null hypothesis.

## T-test
```{r}
ttest_df <- ttest(data_og[,normal_cols], data_cmp[,normal_cols])
ttest_df
```

If the data would be normally distributed, we could also have used the t-test. However, we found earlier this is not the case.
Still, if we execute the code, we can see the same results as above for the non-parametric methods.

## Effect size mean - Cohen's d
```{r}
cohensd_df <- cohensd(data_og[,normal_cols], data_cmp[,normal_cols])
cohensd_df
```

With Cohen's d we show the effect size of the difference in mean. Combining this with the significance found above, we can say something about the actual difference between the datasets. With the conclusions we had drawn above, we can now conclude there is no indications for any difference in the means.

```{r}
latex_table(func=wilc, caption=get_caption("wilc",method_full), label=paste0("tab:",method,"_wilcox"), df1=data_og[,non_normal_cols], df2=data_cmp[,non_normal_cols], paired=TRUE, method="wilcoxon-pratt")
```


```{r}
latex_table(func=ttest_eff, caption=get_caption("ttest",method_full), label=paste0("tab:",method,"_ttest"), df1=data_og[,normal_cols], df2=data_cmp[,normal_cols], paired=TRUE)
```

## Plots
```{r}
plot_line_ordered(data_og, data_cmp)
```

```{r}
trait_boxplots(data_og, data_cmp)
```

```{r}
ggplot_line_ordered(data_og, data_cmp)
```

```{r}
gt <- get_ground_truth()

insp <- inspect_gt_non_zero(data_og, data_cmp, dfnames=dfnames, gt=gt)
insp
```

```{r}
inspect_gt(data_og, data_cmp, dfnames=dfnames, gt=gt)
```

```{r}
diff_cols <- get_diff_cols(insp)

latex_table(inspect_gt, caption=get_caption("accuracy",method_full), label=paste0("tab:",method,"_gt_RMSE_MAE"),
            include_index=FALSE,
            shorten_names=1,
            fix_column=1,
            transpose=TRUE,
            alignment="c",
            df1=data_og, df2=data_cmp, dfnames=dfnames, gt=gt, columns=diff_cols)
```


From all tests above, we conclude the email preprocessing does not worsen nor improve the final outcome. Therefore, we consider this step as necessary as it does remove personally identifable information and, thus, makes the process more aware of the user's privacy.
